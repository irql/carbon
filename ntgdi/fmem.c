

#include "driver.h"

/* i'm aware this is terrible but it should be a little faster than looping and potentially 
	much faster for copying the double buffer to the frame buffer */

void*
memcpy_SSE2_4KA(
	__in void* m1,
	__in void* m2,
	__in __int64 c
	)
{
	__m128i* u1 = m1;
	__m128i* u2 = m2;

	do {
		_mm_store_si128(u1, _mm_load_si128(u2));
		_mm_store_si128(u1 + 1, _mm_load_si128(u2 + 1));
		_mm_store_si128(u1 + 2, _mm_load_si128(u2 + 2));
		_mm_store_si128(u1 + 3, _mm_load_si128(u2 + 3));
		_mm_store_si128(u1 + 4, _mm_load_si128(u2 + 4));
		_mm_store_si128(u1 + 5, _mm_load_si128(u2 + 5));
		_mm_store_si128(u1 + 6, _mm_load_si128(u2 + 6));
		_mm_store_si128(u1 + 7, _mm_load_si128(u2 + 7));
		_mm_store_si128(u1 + 8, _mm_load_si128(u2 + 8));
		_mm_store_si128(u1 + 9, _mm_load_si128(u2 + 9));
		_mm_store_si128(u1 + 10, _mm_load_si128(u2 + 10));
		_mm_store_si128(u1 + 11, _mm_load_si128(u2 + 11));
		_mm_store_si128(u1 + 12, _mm_load_si128(u2 + 12));
		_mm_store_si128(u1 + 13, _mm_load_si128(u2 + 13));
		_mm_store_si128(u1 + 14, _mm_load_si128(u2 + 14));
		_mm_store_si128(u1 + 15, _mm_load_si128(u2 + 15));
		_mm_store_si128(u1 + 16, _mm_load_si128(u2 + 16));
		_mm_store_si128(u1 + 17, _mm_load_si128(u2 + 17));
		_mm_store_si128(u1 + 18, _mm_load_si128(u2 + 18));
		_mm_store_si128(u1 + 19, _mm_load_si128(u2 + 19));
		_mm_store_si128(u1 + 20, _mm_load_si128(u2 + 20));
		_mm_store_si128(u1 + 21, _mm_load_si128(u2 + 21));
		_mm_store_si128(u1 + 22, _mm_load_si128(u2 + 22));
		_mm_store_si128(u1 + 23, _mm_load_si128(u2 + 23));
		_mm_store_si128(u1 + 24, _mm_load_si128(u2 + 24));
		_mm_store_si128(u1 + 25, _mm_load_si128(u2 + 25));
		_mm_store_si128(u1 + 26, _mm_load_si128(u2 + 26));
		_mm_store_si128(u1 + 27, _mm_load_si128(u2 + 27));
		_mm_store_si128(u1 + 28, _mm_load_si128(u2 + 28));
		_mm_store_si128(u1 + 29, _mm_load_si128(u2 + 29));
		_mm_store_si128(u1 + 30, _mm_load_si128(u2 + 30));
		_mm_store_si128(u1 + 31, _mm_load_si128(u2 + 31));
		_mm_store_si128(u1 + 32, _mm_load_si128(u2 + 32));
		_mm_store_si128(u1 + 33, _mm_load_si128(u2 + 33));
		_mm_store_si128(u1 + 34, _mm_load_si128(u2 + 34));
		_mm_store_si128(u1 + 35, _mm_load_si128(u2 + 35));
		_mm_store_si128(u1 + 36, _mm_load_si128(u2 + 36));
		_mm_store_si128(u1 + 37, _mm_load_si128(u2 + 37));
		_mm_store_si128(u1 + 38, _mm_load_si128(u2 + 38));
		_mm_store_si128(u1 + 39, _mm_load_si128(u2 + 39));
		_mm_store_si128(u1 + 40, _mm_load_si128(u2 + 40));
		_mm_store_si128(u1 + 41, _mm_load_si128(u2 + 41));
		_mm_store_si128(u1 + 42, _mm_load_si128(u2 + 42));
		_mm_store_si128(u1 + 43, _mm_load_si128(u2 + 43));
		_mm_store_si128(u1 + 44, _mm_load_si128(u2 + 44));
		_mm_store_si128(u1 + 45, _mm_load_si128(u2 + 45));
		_mm_store_si128(u1 + 46, _mm_load_si128(u2 + 46));
		_mm_store_si128(u1 + 47, _mm_load_si128(u2 + 47));
		_mm_store_si128(u1 + 48, _mm_load_si128(u2 + 48));
		_mm_store_si128(u1 + 49, _mm_load_si128(u2 + 49));
		_mm_store_si128(u1 + 50, _mm_load_si128(u2 + 50));
		_mm_store_si128(u1 + 51, _mm_load_si128(u2 + 51));
		_mm_store_si128(u1 + 52, _mm_load_si128(u2 + 52));
		_mm_store_si128(u1 + 53, _mm_load_si128(u2 + 53));
		_mm_store_si128(u1 + 54, _mm_load_si128(u2 + 54));
		_mm_store_si128(u1 + 55, _mm_load_si128(u2 + 55));
		_mm_store_si128(u1 + 56, _mm_load_si128(u2 + 56));
		_mm_store_si128(u1 + 57, _mm_load_si128(u2 + 57));
		_mm_store_si128(u1 + 58, _mm_load_si128(u2 + 58));
		_mm_store_si128(u1 + 59, _mm_load_si128(u2 + 59));
		_mm_store_si128(u1 + 60, _mm_load_si128(u2 + 60));
		_mm_store_si128(u1 + 61, _mm_load_si128(u2 + 61));
		_mm_store_si128(u1 + 62, _mm_load_si128(u2 + 62));
		_mm_store_si128(u1 + 63, _mm_load_si128(u2 + 63));
		_mm_store_si128(u1 + 64, _mm_load_si128(u2 + 64));
		_mm_store_si128(u1 + 65, _mm_load_si128(u2 + 65));
		_mm_store_si128(u1 + 66, _mm_load_si128(u2 + 66));
		_mm_store_si128(u1 + 67, _mm_load_si128(u2 + 67));
		_mm_store_si128(u1 + 68, _mm_load_si128(u2 + 68));
		_mm_store_si128(u1 + 69, _mm_load_si128(u2 + 69));
		_mm_store_si128(u1 + 70, _mm_load_si128(u2 + 70));
		_mm_store_si128(u1 + 71, _mm_load_si128(u2 + 71));
		_mm_store_si128(u1 + 72, _mm_load_si128(u2 + 72));
		_mm_store_si128(u1 + 73, _mm_load_si128(u2 + 73));
		_mm_store_si128(u1 + 74, _mm_load_si128(u2 + 74));
		_mm_store_si128(u1 + 75, _mm_load_si128(u2 + 75));
		_mm_store_si128(u1 + 76, _mm_load_si128(u2 + 76));
		_mm_store_si128(u1 + 77, _mm_load_si128(u2 + 77));
		_mm_store_si128(u1 + 78, _mm_load_si128(u2 + 78));
		_mm_store_si128(u1 + 79, _mm_load_si128(u2 + 79));
		_mm_store_si128(u1 + 80, _mm_load_si128(u2 + 80));
		_mm_store_si128(u1 + 81, _mm_load_si128(u2 + 81));
		_mm_store_si128(u1 + 82, _mm_load_si128(u2 + 82));
		_mm_store_si128(u1 + 83, _mm_load_si128(u2 + 83));
		_mm_store_si128(u1 + 84, _mm_load_si128(u2 + 84));
		_mm_store_si128(u1 + 85, _mm_load_si128(u2 + 85));
		_mm_store_si128(u1 + 86, _mm_load_si128(u2 + 86));
		_mm_store_si128(u1 + 87, _mm_load_si128(u2 + 87));
		_mm_store_si128(u1 + 88, _mm_load_si128(u2 + 88));
		_mm_store_si128(u1 + 89, _mm_load_si128(u2 + 89));
		_mm_store_si128(u1 + 90, _mm_load_si128(u2 + 90));
		_mm_store_si128(u1 + 91, _mm_load_si128(u2 + 91));
		_mm_store_si128(u1 + 92, _mm_load_si128(u2 + 92));
		_mm_store_si128(u1 + 93, _mm_load_si128(u2 + 93));
		_mm_store_si128(u1 + 94, _mm_load_si128(u2 + 94));
		_mm_store_si128(u1 + 95, _mm_load_si128(u2 + 95));
		_mm_store_si128(u1 + 96, _mm_load_si128(u2 + 96));
		_mm_store_si128(u1 + 97, _mm_load_si128(u2 + 97));
		_mm_store_si128(u1 + 98, _mm_load_si128(u2 + 98));
		_mm_store_si128(u1 + 99, _mm_load_si128(u2 + 99));
		_mm_store_si128(u1 + 100, _mm_load_si128(u2 + 100));
		_mm_store_si128(u1 + 101, _mm_load_si128(u2 + 101));
		_mm_store_si128(u1 + 102, _mm_load_si128(u2 + 102));
		_mm_store_si128(u1 + 103, _mm_load_si128(u2 + 103));
		_mm_store_si128(u1 + 104, _mm_load_si128(u2 + 104));
		_mm_store_si128(u1 + 105, _mm_load_si128(u2 + 105));
		_mm_store_si128(u1 + 106, _mm_load_si128(u2 + 106));
		_mm_store_si128(u1 + 107, _mm_load_si128(u2 + 107));
		_mm_store_si128(u1 + 108, _mm_load_si128(u2 + 108));
		_mm_store_si128(u1 + 109, _mm_load_si128(u2 + 109));
		_mm_store_si128(u1 + 110, _mm_load_si128(u2 + 110));
		_mm_store_si128(u1 + 111, _mm_load_si128(u2 + 111));
		_mm_store_si128(u1 + 112, _mm_load_si128(u2 + 112));
		_mm_store_si128(u1 + 113, _mm_load_si128(u2 + 113));
		_mm_store_si128(u1 + 114, _mm_load_si128(u2 + 114));
		_mm_store_si128(u1 + 115, _mm_load_si128(u2 + 115));
		_mm_store_si128(u1 + 116, _mm_load_si128(u2 + 116));
		_mm_store_si128(u1 + 117, _mm_load_si128(u2 + 117));
		_mm_store_si128(u1 + 118, _mm_load_si128(u2 + 118));
		_mm_store_si128(u1 + 119, _mm_load_si128(u2 + 119));
		_mm_store_si128(u1 + 120, _mm_load_si128(u2 + 120));
		_mm_store_si128(u1 + 121, _mm_load_si128(u2 + 121));
		_mm_store_si128(u1 + 122, _mm_load_si128(u2 + 122));
		_mm_store_si128(u1 + 123, _mm_load_si128(u2 + 123));
		_mm_store_si128(u1 + 124, _mm_load_si128(u2 + 124));
		_mm_store_si128(u1 + 125, _mm_load_si128(u2 + 125));
		_mm_store_si128(u1 + 126, _mm_load_si128(u2 + 126));
		_mm_store_si128(u1 + 127, _mm_load_si128(u2 + 127));
		_mm_store_si128(u1 + 128, _mm_load_si128(u2 + 128));
		_mm_store_si128(u1 + 129, _mm_load_si128(u2 + 129));
		_mm_store_si128(u1 + 130, _mm_load_si128(u2 + 130));
		_mm_store_si128(u1 + 131, _mm_load_si128(u2 + 131));
		_mm_store_si128(u1 + 132, _mm_load_si128(u2 + 132));
		_mm_store_si128(u1 + 133, _mm_load_si128(u2 + 133));
		_mm_store_si128(u1 + 134, _mm_load_si128(u2 + 134));
		_mm_store_si128(u1 + 135, _mm_load_si128(u2 + 135));
		_mm_store_si128(u1 + 136, _mm_load_si128(u2 + 136));
		_mm_store_si128(u1 + 137, _mm_load_si128(u2 + 137));
		_mm_store_si128(u1 + 138, _mm_load_si128(u2 + 138));
		_mm_store_si128(u1 + 139, _mm_load_si128(u2 + 139));
		_mm_store_si128(u1 + 140, _mm_load_si128(u2 + 140));
		_mm_store_si128(u1 + 141, _mm_load_si128(u2 + 141));
		_mm_store_si128(u1 + 142, _mm_load_si128(u2 + 142));
		_mm_store_si128(u1 + 143, _mm_load_si128(u2 + 143));
		_mm_store_si128(u1 + 144, _mm_load_si128(u2 + 144));
		_mm_store_si128(u1 + 145, _mm_load_si128(u2 + 145));
		_mm_store_si128(u1 + 146, _mm_load_si128(u2 + 146));
		_mm_store_si128(u1 + 147, _mm_load_si128(u2 + 147));
		_mm_store_si128(u1 + 148, _mm_load_si128(u2 + 148));
		_mm_store_si128(u1 + 149, _mm_load_si128(u2 + 149));
		_mm_store_si128(u1 + 150, _mm_load_si128(u2 + 150));
		_mm_store_si128(u1 + 151, _mm_load_si128(u2 + 151));
		_mm_store_si128(u1 + 152, _mm_load_si128(u2 + 152));
		_mm_store_si128(u1 + 153, _mm_load_si128(u2 + 153));
		_mm_store_si128(u1 + 154, _mm_load_si128(u2 + 154));
		_mm_store_si128(u1 + 155, _mm_load_si128(u2 + 155));
		_mm_store_si128(u1 + 156, _mm_load_si128(u2 + 156));
		_mm_store_si128(u1 + 157, _mm_load_si128(u2 + 157));
		_mm_store_si128(u1 + 158, _mm_load_si128(u2 + 158));
		_mm_store_si128(u1 + 159, _mm_load_si128(u2 + 159));
		_mm_store_si128(u1 + 160, _mm_load_si128(u2 + 160));
		_mm_store_si128(u1 + 161, _mm_load_si128(u2 + 161));
		_mm_store_si128(u1 + 162, _mm_load_si128(u2 + 162));
		_mm_store_si128(u1 + 163, _mm_load_si128(u2 + 163));
		_mm_store_si128(u1 + 164, _mm_load_si128(u2 + 164));
		_mm_store_si128(u1 + 165, _mm_load_si128(u2 + 165));
		_mm_store_si128(u1 + 166, _mm_load_si128(u2 + 166));
		_mm_store_si128(u1 + 167, _mm_load_si128(u2 + 167));
		_mm_store_si128(u1 + 168, _mm_load_si128(u2 + 168));
		_mm_store_si128(u1 + 169, _mm_load_si128(u2 + 169));
		_mm_store_si128(u1 + 170, _mm_load_si128(u2 + 170));
		_mm_store_si128(u1 + 171, _mm_load_si128(u2 + 171));
		_mm_store_si128(u1 + 172, _mm_load_si128(u2 + 172));
		_mm_store_si128(u1 + 173, _mm_load_si128(u2 + 173));
		_mm_store_si128(u1 + 174, _mm_load_si128(u2 + 174));
		_mm_store_si128(u1 + 175, _mm_load_si128(u2 + 175));
		_mm_store_si128(u1 + 176, _mm_load_si128(u2 + 176));
		_mm_store_si128(u1 + 177, _mm_load_si128(u2 + 177));
		_mm_store_si128(u1 + 178, _mm_load_si128(u2 + 178));
		_mm_store_si128(u1 + 179, _mm_load_si128(u2 + 179));
		_mm_store_si128(u1 + 180, _mm_load_si128(u2 + 180));
		_mm_store_si128(u1 + 181, _mm_load_si128(u2 + 181));
		_mm_store_si128(u1 + 182, _mm_load_si128(u2 + 182));
		_mm_store_si128(u1 + 183, _mm_load_si128(u2 + 183));
		_mm_store_si128(u1 + 184, _mm_load_si128(u2 + 184));
		_mm_store_si128(u1 + 185, _mm_load_si128(u2 + 185));
		_mm_store_si128(u1 + 186, _mm_load_si128(u2 + 186));
		_mm_store_si128(u1 + 187, _mm_load_si128(u2 + 187));
		_mm_store_si128(u1 + 188, _mm_load_si128(u2 + 188));
		_mm_store_si128(u1 + 189, _mm_load_si128(u2 + 189));
		_mm_store_si128(u1 + 190, _mm_load_si128(u2 + 190));
		_mm_store_si128(u1 + 191, _mm_load_si128(u2 + 191));
		_mm_store_si128(u1 + 192, _mm_load_si128(u2 + 192));
		_mm_store_si128(u1 + 193, _mm_load_si128(u2 + 193));
		_mm_store_si128(u1 + 194, _mm_load_si128(u2 + 194));
		_mm_store_si128(u1 + 195, _mm_load_si128(u2 + 195));
		_mm_store_si128(u1 + 196, _mm_load_si128(u2 + 196));
		_mm_store_si128(u1 + 197, _mm_load_si128(u2 + 197));
		_mm_store_si128(u1 + 198, _mm_load_si128(u2 + 198));
		_mm_store_si128(u1 + 199, _mm_load_si128(u2 + 199));
		_mm_store_si128(u1 + 200, _mm_load_si128(u2 + 200));
		_mm_store_si128(u1 + 201, _mm_load_si128(u2 + 201));
		_mm_store_si128(u1 + 202, _mm_load_si128(u2 + 202));
		_mm_store_si128(u1 + 203, _mm_load_si128(u2 + 203));
		_mm_store_si128(u1 + 204, _mm_load_si128(u2 + 204));
		_mm_store_si128(u1 + 205, _mm_load_si128(u2 + 205));
		_mm_store_si128(u1 + 206, _mm_load_si128(u2 + 206));
		_mm_store_si128(u1 + 207, _mm_load_si128(u2 + 207));
		_mm_store_si128(u1 + 208, _mm_load_si128(u2 + 208));
		_mm_store_si128(u1 + 209, _mm_load_si128(u2 + 209));
		_mm_store_si128(u1 + 210, _mm_load_si128(u2 + 210));
		_mm_store_si128(u1 + 211, _mm_load_si128(u2 + 211));
		_mm_store_si128(u1 + 212, _mm_load_si128(u2 + 212));
		_mm_store_si128(u1 + 213, _mm_load_si128(u2 + 213));
		_mm_store_si128(u1 + 214, _mm_load_si128(u2 + 214));
		_mm_store_si128(u1 + 215, _mm_load_si128(u2 + 215));
		_mm_store_si128(u1 + 216, _mm_load_si128(u2 + 216));
		_mm_store_si128(u1 + 217, _mm_load_si128(u2 + 217));
		_mm_store_si128(u1 + 218, _mm_load_si128(u2 + 218));
		_mm_store_si128(u1 + 219, _mm_load_si128(u2 + 219));
		_mm_store_si128(u1 + 220, _mm_load_si128(u2 + 220));
		_mm_store_si128(u1 + 221, _mm_load_si128(u2 + 221));
		_mm_store_si128(u1 + 222, _mm_load_si128(u2 + 222));
		_mm_store_si128(u1 + 223, _mm_load_si128(u2 + 223));
		_mm_store_si128(u1 + 224, _mm_load_si128(u2 + 224));
		_mm_store_si128(u1 + 225, _mm_load_si128(u2 + 225));
		_mm_store_si128(u1 + 226, _mm_load_si128(u2 + 226));
		_mm_store_si128(u1 + 227, _mm_load_si128(u2 + 227));
		_mm_store_si128(u1 + 228, _mm_load_si128(u2 + 228));
		_mm_store_si128(u1 + 229, _mm_load_si128(u2 + 229));
		_mm_store_si128(u1 + 230, _mm_load_si128(u2 + 230));
		_mm_store_si128(u1 + 231, _mm_load_si128(u2 + 231));
		_mm_store_si128(u1 + 232, _mm_load_si128(u2 + 232));
		_mm_store_si128(u1 + 233, _mm_load_si128(u2 + 233));
		_mm_store_si128(u1 + 234, _mm_load_si128(u2 + 234));
		_mm_store_si128(u1 + 235, _mm_load_si128(u2 + 235));
		_mm_store_si128(u1 + 236, _mm_load_si128(u2 + 236));
		_mm_store_si128(u1 + 237, _mm_load_si128(u2 + 237));
		_mm_store_si128(u1 + 238, _mm_load_si128(u2 + 238));
		_mm_store_si128(u1 + 239, _mm_load_si128(u2 + 239));
		_mm_store_si128(u1 + 240, _mm_load_si128(u2 + 240));
		_mm_store_si128(u1 + 241, _mm_load_si128(u2 + 241));
		_mm_store_si128(u1 + 242, _mm_load_si128(u2 + 242));
		_mm_store_si128(u1 + 243, _mm_load_si128(u2 + 243));
		_mm_store_si128(u1 + 244, _mm_load_si128(u2 + 244));
		_mm_store_si128(u1 + 245, _mm_load_si128(u2 + 245));
		_mm_store_si128(u1 + 246, _mm_load_si128(u2 + 246));
		_mm_store_si128(u1 + 247, _mm_load_si128(u2 + 247));
		_mm_store_si128(u1 + 248, _mm_load_si128(u2 + 248));
		_mm_store_si128(u1 + 249, _mm_load_si128(u2 + 249));
		_mm_store_si128(u1 + 250, _mm_load_si128(u2 + 250));
		_mm_store_si128(u1 + 251, _mm_load_si128(u2 + 251));
		_mm_store_si128(u1 + 252, _mm_load_si128(u2 + 252));
		_mm_store_si128(u1 + 253, _mm_load_si128(u2 + 253));
		_mm_store_si128(u1 + 254, _mm_load_si128(u2 + 254));
		_mm_store_si128(u1 + 255, _mm_load_si128(u2 + 255));
		u1 += 256;
		u2 += 256;
		c--;
	} while (c);

	return m1;
}

void*
memset_SSE2_4KA(
	__in void* m1,
	__in __int64 v,
	__in __int64 c
	)
{
	__m128i*u1 = m1;
	__m128i v1 = _mm_cvtsi64_si128(v);
	__m128i v2 = _mm_unpacklo_epi64(v1, v1);

	do {
		_mm_store_si128(u1, v2);
		_mm_store_si128(u1 + 1, v2);
		_mm_store_si128(u1 + 2, v2);
		_mm_store_si128(u1 + 3, v2);
		_mm_store_si128(u1 + 4, v2);
		_mm_store_si128(u1 + 5, v2);
		_mm_store_si128(u1 + 6, v2);
		_mm_store_si128(u1 + 7, v2);
		_mm_store_si128(u1 + 8, v2);
		_mm_store_si128(u1 + 9, v2);
		_mm_store_si128(u1 + 10, v2);
		_mm_store_si128(u1 + 11, v2);
		_mm_store_si128(u1 + 12, v2);
		_mm_store_si128(u1 + 13, v2);
		_mm_store_si128(u1 + 14, v2);
		_mm_store_si128(u1 + 15, v2);
		_mm_store_si128(u1 + 16, v2);
		_mm_store_si128(u1 + 17, v2);
		_mm_store_si128(u1 + 18, v2);
		_mm_store_si128(u1 + 19, v2);
		_mm_store_si128(u1 + 20, v2);
		_mm_store_si128(u1 + 21, v2);
		_mm_store_si128(u1 + 22, v2);
		_mm_store_si128(u1 + 23, v2);
		_mm_store_si128(u1 + 24, v2);
		_mm_store_si128(u1 + 25, v2);
		_mm_store_si128(u1 + 26, v2);
		_mm_store_si128(u1 + 27, v2);
		_mm_store_si128(u1 + 28, v2);
		_mm_store_si128(u1 + 29, v2);
		_mm_store_si128(u1 + 30, v2);
		_mm_store_si128(u1 + 31, v2);
		_mm_store_si128(u1 + 32, v2);
		_mm_store_si128(u1 + 33, v2);
		_mm_store_si128(u1 + 34, v2);
		_mm_store_si128(u1 + 35, v2);
		_mm_store_si128(u1 + 36, v2);
		_mm_store_si128(u1 + 37, v2);
		_mm_store_si128(u1 + 38, v2);
		_mm_store_si128(u1 + 39, v2);
		_mm_store_si128(u1 + 40, v2);
		_mm_store_si128(u1 + 41, v2);
		_mm_store_si128(u1 + 42, v2);
		_mm_store_si128(u1 + 43, v2);
		_mm_store_si128(u1 + 44, v2);
		_mm_store_si128(u1 + 45, v2);
		_mm_store_si128(u1 + 46, v2);
		_mm_store_si128(u1 + 47, v2);
		_mm_store_si128(u1 + 48, v2);
		_mm_store_si128(u1 + 49, v2);
		_mm_store_si128(u1 + 50, v2);
		_mm_store_si128(u1 + 51, v2);
		_mm_store_si128(u1 + 52, v2);
		_mm_store_si128(u1 + 53, v2);
		_mm_store_si128(u1 + 54, v2);
		_mm_store_si128(u1 + 55, v2);
		_mm_store_si128(u1 + 56, v2);
		_mm_store_si128(u1 + 57, v2);
		_mm_store_si128(u1 + 58, v2);
		_mm_store_si128(u1 + 59, v2);
		_mm_store_si128(u1 + 60, v2);
		_mm_store_si128(u1 + 61, v2);
		_mm_store_si128(u1 + 62, v2);
		_mm_store_si128(u1 + 63, v2);
		_mm_store_si128(u1 + 64, v2);
		_mm_store_si128(u1 + 65, v2);
		_mm_store_si128(u1 + 66, v2);
		_mm_store_si128(u1 + 67, v2);
		_mm_store_si128(u1 + 68, v2);
		_mm_store_si128(u1 + 69, v2);
		_mm_store_si128(u1 + 70, v2);
		_mm_store_si128(u1 + 71, v2);
		_mm_store_si128(u1 + 72, v2);
		_mm_store_si128(u1 + 73, v2);
		_mm_store_si128(u1 + 74, v2);
		_mm_store_si128(u1 + 75, v2);
		_mm_store_si128(u1 + 76, v2);
		_mm_store_si128(u1 + 77, v2);
		_mm_store_si128(u1 + 78, v2);
		_mm_store_si128(u1 + 79, v2);
		_mm_store_si128(u1 + 80, v2);
		_mm_store_si128(u1 + 81, v2);
		_mm_store_si128(u1 + 82, v2);
		_mm_store_si128(u1 + 83, v2);
		_mm_store_si128(u1 + 84, v2);
		_mm_store_si128(u1 + 85, v2);
		_mm_store_si128(u1 + 86, v2);
		_mm_store_si128(u1 + 87, v2);
		_mm_store_si128(u1 + 88, v2);
		_mm_store_si128(u1 + 89, v2);
		_mm_store_si128(u1 + 90, v2);
		_mm_store_si128(u1 + 91, v2);
		_mm_store_si128(u1 + 92, v2);
		_mm_store_si128(u1 + 93, v2);
		_mm_store_si128(u1 + 94, v2);
		_mm_store_si128(u1 + 95, v2);
		_mm_store_si128(u1 + 96, v2);
		_mm_store_si128(u1 + 97, v2);
		_mm_store_si128(u1 + 98, v2);
		_mm_store_si128(u1 + 99, v2);
		_mm_store_si128(u1 + 100, v2);
		_mm_store_si128(u1 + 101, v2);
		_mm_store_si128(u1 + 102, v2);
		_mm_store_si128(u1 + 103, v2);
		_mm_store_si128(u1 + 104, v2);
		_mm_store_si128(u1 + 105, v2);
		_mm_store_si128(u1 + 106, v2);
		_mm_store_si128(u1 + 107, v2);
		_mm_store_si128(u1 + 108, v2);
		_mm_store_si128(u1 + 109, v2);
		_mm_store_si128(u1 + 110, v2);
		_mm_store_si128(u1 + 111, v2);
		_mm_store_si128(u1 + 112, v2);
		_mm_store_si128(u1 + 113, v2);
		_mm_store_si128(u1 + 114, v2);
		_mm_store_si128(u1 + 115, v2);
		_mm_store_si128(u1 + 116, v2);
		_mm_store_si128(u1 + 117, v2);
		_mm_store_si128(u1 + 118, v2);
		_mm_store_si128(u1 + 119, v2);
		_mm_store_si128(u1 + 120, v2);
		_mm_store_si128(u1 + 121, v2);
		_mm_store_si128(u1 + 122, v2);
		_mm_store_si128(u1 + 123, v2);
		_mm_store_si128(u1 + 124, v2);
		_mm_store_si128(u1 + 125, v2);
		_mm_store_si128(u1 + 126, v2);
		_mm_store_si128(u1 + 127, v2);
		_mm_store_si128(u1 + 128, v2);
		_mm_store_si128(u1 + 129, v2);
		_mm_store_si128(u1 + 130, v2);
		_mm_store_si128(u1 + 131, v2);
		_mm_store_si128(u1 + 132, v2);
		_mm_store_si128(u1 + 133, v2);
		_mm_store_si128(u1 + 134, v2);
		_mm_store_si128(u1 + 135, v2);
		_mm_store_si128(u1 + 136, v2);
		_mm_store_si128(u1 + 137, v2);
		_mm_store_si128(u1 + 138, v2);
		_mm_store_si128(u1 + 139, v2);
		_mm_store_si128(u1 + 140, v2);
		_mm_store_si128(u1 + 141, v2);
		_mm_store_si128(u1 + 142, v2);
		_mm_store_si128(u1 + 143, v2);
		_mm_store_si128(u1 + 144, v2);
		_mm_store_si128(u1 + 145, v2);
		_mm_store_si128(u1 + 146, v2);
		_mm_store_si128(u1 + 147, v2);
		_mm_store_si128(u1 + 148, v2);
		_mm_store_si128(u1 + 149, v2);
		_mm_store_si128(u1 + 150, v2);
		_mm_store_si128(u1 + 151, v2);
		_mm_store_si128(u1 + 152, v2);
		_mm_store_si128(u1 + 153, v2);
		_mm_store_si128(u1 + 154, v2);
		_mm_store_si128(u1 + 155, v2);
		_mm_store_si128(u1 + 156, v2);
		_mm_store_si128(u1 + 157, v2);
		_mm_store_si128(u1 + 158, v2);
		_mm_store_si128(u1 + 159, v2);
		_mm_store_si128(u1 + 160, v2);
		_mm_store_si128(u1 + 161, v2);
		_mm_store_si128(u1 + 162, v2);
		_mm_store_si128(u1 + 163, v2);
		_mm_store_si128(u1 + 164, v2);
		_mm_store_si128(u1 + 165, v2);
		_mm_store_si128(u1 + 166, v2);
		_mm_store_si128(u1 + 167, v2);
		_mm_store_si128(u1 + 168, v2);
		_mm_store_si128(u1 + 169, v2);
		_mm_store_si128(u1 + 170, v2);
		_mm_store_si128(u1 + 171, v2);
		_mm_store_si128(u1 + 172, v2);
		_mm_store_si128(u1 + 173, v2);
		_mm_store_si128(u1 + 174, v2);
		_mm_store_si128(u1 + 175, v2);
		_mm_store_si128(u1 + 176, v2);
		_mm_store_si128(u1 + 177, v2);
		_mm_store_si128(u1 + 178, v2);
		_mm_store_si128(u1 + 179, v2);
		_mm_store_si128(u1 + 180, v2);
		_mm_store_si128(u1 + 181, v2);
		_mm_store_si128(u1 + 182, v2);
		_mm_store_si128(u1 + 183, v2);
		_mm_store_si128(u1 + 184, v2);
		_mm_store_si128(u1 + 185, v2);
		_mm_store_si128(u1 + 186, v2);
		_mm_store_si128(u1 + 187, v2);
		_mm_store_si128(u1 + 188, v2);
		_mm_store_si128(u1 + 189, v2);
		_mm_store_si128(u1 + 190, v2);
		_mm_store_si128(u1 + 191, v2);
		_mm_store_si128(u1 + 192, v2);
		_mm_store_si128(u1 + 193, v2);
		_mm_store_si128(u1 + 194, v2);
		_mm_store_si128(u1 + 195, v2);
		_mm_store_si128(u1 + 196, v2);
		_mm_store_si128(u1 + 197, v2);
		_mm_store_si128(u1 + 198, v2);
		_mm_store_si128(u1 + 199, v2);
		_mm_store_si128(u1 + 200, v2);
		_mm_store_si128(u1 + 201, v2);
		_mm_store_si128(u1 + 202, v2);
		_mm_store_si128(u1 + 203, v2);
		_mm_store_si128(u1 + 204, v2);
		_mm_store_si128(u1 + 205, v2);
		_mm_store_si128(u1 + 206, v2);
		_mm_store_si128(u1 + 207, v2);
		_mm_store_si128(u1 + 208, v2);
		_mm_store_si128(u1 + 209, v2);
		_mm_store_si128(u1 + 210, v2);
		_mm_store_si128(u1 + 211, v2);
		_mm_store_si128(u1 + 212, v2);
		_mm_store_si128(u1 + 213, v2);
		_mm_store_si128(u1 + 214, v2);
		_mm_store_si128(u1 + 215, v2);
		_mm_store_si128(u1 + 216, v2);
		_mm_store_si128(u1 + 217, v2);
		_mm_store_si128(u1 + 218, v2);
		_mm_store_si128(u1 + 219, v2);
		_mm_store_si128(u1 + 220, v2);
		_mm_store_si128(u1 + 221, v2);
		_mm_store_si128(u1 + 222, v2);
		_mm_store_si128(u1 + 223, v2);
		_mm_store_si128(u1 + 224, v2);
		_mm_store_si128(u1 + 225, v2);
		_mm_store_si128(u1 + 226, v2);
		_mm_store_si128(u1 + 227, v2);
		_mm_store_si128(u1 + 228, v2);
		_mm_store_si128(u1 + 229, v2);
		_mm_store_si128(u1 + 230, v2);
		_mm_store_si128(u1 + 231, v2);
		_mm_store_si128(u1 + 232, v2);
		_mm_store_si128(u1 + 233, v2);
		_mm_store_si128(u1 + 234, v2);
		_mm_store_si128(u1 + 235, v2);
		_mm_store_si128(u1 + 236, v2);
		_mm_store_si128(u1 + 237, v2);
		_mm_store_si128(u1 + 238, v2);
		_mm_store_si128(u1 + 239, v2);
		_mm_store_si128(u1 + 240, v2);
		_mm_store_si128(u1 + 241, v2);
		_mm_store_si128(u1 + 242, v2);
		_mm_store_si128(u1 + 243, v2);
		_mm_store_si128(u1 + 244, v2);
		_mm_store_si128(u1 + 245, v2);
		_mm_store_si128(u1 + 246, v2);
		_mm_store_si128(u1 + 247, v2);
		_mm_store_si128(u1 + 248, v2);
		_mm_store_si128(u1 + 249, v2);
		_mm_store_si128(u1 + 250, v2);
		_mm_store_si128(u1 + 251, v2);
		_mm_store_si128(u1 + 252, v2);
		_mm_store_si128(u1 + 253, v2);
		_mm_store_si128(u1 + 254, v2);
		_mm_store_si128(u1 + 255, v2);
		u1 += 256;
		c--;
	} while (c);

	return m1;
}
